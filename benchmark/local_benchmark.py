#!/usr/bin/env python3
"""
ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬ ë° ì±„íŒ… ìŠ¤í¬ë¦½íŠ¸
ì„œë²„ ì—†ì´ ì§ì ‘ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê±°ë‚˜ ì±„íŒ…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
import os
import sys
import time
import gc
from typing import Optional, Dict, List

# UnicodeEncodeError ë°©ì§€
if sys.stdout.encoding != 'utf-8':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except:
        pass

try:
    from llama_cpp import Llama
except ImportError:
    print("âœ— llama-cpp-python is not installed. Install it with: pip install llama-cpp-python")
    sys.exit(1)

try:
    from huggingface_hub import hf_hub_download
except ImportError:
    print("âœ— huggingface-hub is not installed. Install it with: pip install huggingface-hub")
    sys.exit(1)

# ëª¨ë¸ ì„¤ì •
MODELS = {
    "llama32b": {
        "name": "Llama-3.2-3B-Instruct-Q4_K_M",
        "repo_id": "bartowski/Llama-3.2-3B-Instruct-GGUF",
        "filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "chat_format": "llama-3",
        "system_prompt": "Answer in about 10 words or less."
    },
    "tinllama": {
        "name": "TinyLlama-1.1B-Chat-Q4_K_M",
        "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
        "filename": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "chat_format": "chatml",  # TinyLlamaì€ chatml í¬ë§· ì‚¬ìš©
        "system_prompt": "You are a helpful assistant."
    },
    "tinllama4q": {
        "name": "TinyLlama-1.1B-Chat-Q4_0",
        "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
        "filename": "tinyllama-1.1b-chat-v1.0.Q4_0.gguf",
        "chat_format": "chatml",  # TinyLlamaì€ chatml í¬ë§· ì‚¬ìš©
        "system_prompt": "You are a helpful assistant."
    }
}

# í…ŒìŠ¤íŠ¸ ì„¤ì •
QUESTION = "who are you?"

def get_model_path(repo_id: str, filename: str) -> str:
    """ëª¨ë¸ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ë‹¤ìš´ë¡œë“œ (ëŸ°íƒ€ì„)"""
    cache_dir = os.getenv('HF_CACHE_DIR', os.path.expanduser('~/.cache/huggingface/hub'))
    print(f"ëª¨ë¸ í™•ì¸ ì¤‘: {repo_id}/{filename}")
    
    try:
        model_path = hf_hub_download(
            repo_id=repo_id,
            filename=filename,
            cache_dir=cache_dir
        )
        print(f"âœ“ ëª¨ë¸ ê²½ë¡œ: {model_path}")
        return model_path
    except Exception as e:
        print(f"âœ— ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def format_prompt(model_key: str, question: str, system_prompt: str = None) -> str:
    """ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í¬ë§· ìƒì„±"""
    model_config = MODELS[model_key]
    if system_prompt is None:
        system_prompt = model_config["system_prompt"]
    
    chat_format = model_config["chat_format"]
    
    if chat_format == "llama-3":
        return f"<|system|>\n{system_prompt}<|end|>\n<|user|>\n{question}<|end|>\n<|assistant|>\n"
    elif chat_format == "chatml":
        return f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    else:
        return f"{system_prompt}\n\nUser: {question}\nAssistant: "

def run_benchmark_with_model(model_key: str, model: Llama, embedding: bool = False, n_threads: int = 4) -> Dict:
    """ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ë¡œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    model_config = MODELS[model_key]
    embedding_str = "í™œì„±í™”" if embedding else "ë¹„í™œì„±í™”"
    print(f"\n{'='*60}")
    print(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ - {model_config['name']}")
    print(f"ì„ë² ë”©: {embedding_str}, ìŠ¤ë ˆë“œ: {n_threads}")
    print(f"{'='*60}")
    
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print(f"[1/2] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        print(f"  ì§ˆë¬¸: {QUESTION}")
        print(f"  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {model_config['system_prompt']}")
        
        prompt = format_prompt(model_key, QUESTION)
        test_start = time.time()
        output = model(
            prompt,
            max_tokens=20,
            temperature=0.7,
            top_p=0.9,
            echo=False,
            stop=["<|end|>", "<|im_end|>", "\n\n"]
        )
        test_time = time.time() - test_start
        
        # ì‘ë‹µ ì¶”ì¶œ
        if hasattr(output, 'choices'):
            response = output.choices[0].text.strip()
        else:
            response = output['choices'][0]['text'].strip()
        
        print(f"âœ“ ì‘ë‹µ: {response}")
        print(f"âœ“ ì‘ë‹µ ì‹œê°„: {test_time:.3f}ì´ˆ")
        
        # ì„ë² ë”© í…ŒìŠ¤íŠ¸ (í™œì„±í™”ëœ ê²½ìš°)
        embedding_info = None
        if embedding:
            print(f"[2/2] ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
            try:
                response_tokens = model.tokenize(response.encode('utf-8'))
                if response_tokens:
                    first_token_str = model.detokenize([response_tokens[0]]).decode('utf-8', errors='replace')
                    first_token_emb = model.embed(first_token_str)
                    
                    if hasattr(first_token_emb, 'tolist'):
                        emb_list = first_token_emb.tolist()
                    elif isinstance(first_token_emb, list):
                        emb_list = first_token_emb
                    else:
                        emb_list = list(first_token_emb)
                    
                    dim = len(emb_list)
                    # ì• 3ê°œë§Œ ìƒ˜í”Œë¡œ ì €ì¥ (í‰íƒ„í™”)
                    if isinstance(emb_list, list) and len(emb_list) > 0 and isinstance(emb_list[0], list):
                        # ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í‰íƒ„í™”
                        flat_list = [item for sublist in emb_list for item in (sublist if isinstance(sublist, list) else [sublist])]
                        sample = flat_list[:3]
                    else:
                        sample = emb_list[:3] if len(emb_list) >= 3 else emb_list
                    
                    embedding_info = {
                        "token": first_token_str,
                        "dim": dim,
                        "sample": sample  # ì• 3ê°œë§Œ
                    }
                    print(f"âœ“ ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ: {first_token_str} (dim={dim})")
            except Exception as e:
                print(f"âš  ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        else:
            print(f"[2/2] ì„ë² ë”© í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€ (ë¹„í™œì„±í™”)")
        
        result = {
            "model_key": model_key,
            "model_name": model_config["name"],
            "embedding": embedding,
            "n_threads": n_threads,
            "load_time": 0.0,  # ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë¯€ë¡œ 0
            "response_time": test_time,
            "response": response,
            "embedding_info": embedding_info
        }
        
        return result
        
    except Exception as e:
        print(f"âœ— ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def multi_model_chat(model_keys: List[str], embedding: bool = False, n_threads: int = 4, system_prompt: str = None):
    """ì—¬ëŸ¬ ëª¨ë¸ì„ ë™ì‹œì— ë¡œë“œí•˜ì—¬ í•œ ë²ˆì˜ ì…ë ¥ì— ëŒ€í•´ ëª¨ë“  ëª¨ë¸ì´ ë‹µë³€í•˜ëŠ” ì±„íŒ… ëª¨ë“œ"""
    if system_prompt is None:
        system_prompt = "Please respond in about 10 words or less."
    
    print(f"\n{'='*80}")
    print(f"  ë©€í‹° ëª¨ë¸ ì±„íŒ… ëª¨ë“œ")
    print(f"{'='*80}")
    print(f"  ëª¨ë¸: {', '.join([MODELS[key]['name'] for key in model_keys])}")
    print(f"  ì„ë² ë”©: {'í™œì„±í™”' if embedding else 'ë¹„í™œì„±í™”'}, ìŠ¤ë ˆë“œ: {n_threads}")
    print(f"\n  ğŸ“Œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ëª¨ë“  ëª¨ë¸ì— ì ìš©):")
    print(f"     \"{system_prompt}\"")
    print(f"{'='*80}")
    print("  ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', ë˜ëŠ” 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("-"*80)
    
    models = {}
    try:
        # ëª¨ë“  ëª¨ë¸ ë¡œë“œ
        print("\nëª¨ë“  ëª¨ë¸ ë¡œë“œ ì¤‘...")
        for model_key in model_keys:
            model_config = MODELS[model_key]
            print(f"  [{model_config['name']}] ë¡œë“œ ì¤‘...")
            model_path = get_model_path(model_config["repo_id"], model_config["filename"])
            models[model_key] = Llama(
                model_path=model_path,
                n_ctx=4096,
                n_threads=n_threads,
                n_gpu_layers=0,
                chat_format=model_config["chat_format"],
                embedding=embedding,
                verbose=False
            )
            print(f"  âœ“ [{model_config['name']}] ë¡œë“œ ì™„ë£Œ")
        print("\nâœ“ ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n")
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input("\nYou: ").strip()
                
                if not user_input:
                    continue
                
                # ì¢…ë£Œ ëª…ë ¹
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                # ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ë‹µë³€ ìƒì„±
                print(f"\n{'='*80}")
                print(f"  ì§ˆë¬¸: {user_input}")
                print(f"  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: \"{system_prompt}\"")
                print(f"{'='*80}")
                for model_key in model_keys:
                    model_config = MODELS[model_key]
                    model = models[model_key]
                    
                    print(f"\n  [{model_config['name']}]: ", end="", flush=True)
                    prompt = format_prompt(model_key, user_input, system_prompt)
                    start_time = time.time()
                    
                    output = model(
                        prompt,
                        max_tokens=100,
                        temperature=0.7,
                        top_p=0.9,
                        echo=False,
                        stop=["<|end|>", "<|im_end|>", "\n\n"]
                    )
                    
                    elapsed = time.time() - start_time
                    
                    if hasattr(output, 'choices'):
                        response = output.choices[0].text.strip()
                    else:
                        response = output['choices'][0]['text'].strip()
                    
                    print(response)
                    print(f"    (ì‘ë‹µ ì‹œê°„: {elapsed:.3f}ì´ˆ)")
                print(f"{'='*80}")
                
            except KeyboardInterrupt:
                print("\n\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except EOFError:
                print("\n\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâš  ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
        
    except Exception as e:
        print(f"âœ— ë©€í‹° ëª¨ë¸ ì±„íŒ… ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
        print("\nëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
        for model_key, model in models.items():
            del model
        gc.collect()
        print("âœ“ ì •ë¦¬ ì™„ë£Œ")

def interactive_chat(model_key: str, embedding: bool = False, n_threads: int = 4, system_prompt: str = None):
    """ì¸í„°ë™í‹°ë¸Œ ì±„íŒ… ëª¨ë“œ"""
    model_config = MODELS[model_key]
    # ì±„íŒ… ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ í†µì¼ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    if system_prompt is None:
        system_prompt = "Please respond in about 10 words or less."
    
    print(f"\n{'='*60}")
    print(f"ì¸í„°ë™í‹°ë¸Œ ì±„íŒ… ëª¨ë“œ - {model_config['name']}")
    print(f"ì„ë² ë”©: {'í™œì„±í™”' if embedding else 'ë¹„í™œì„±í™”'}, ìŠ¤ë ˆë“œ: {n_threads}")
    print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt}")
    print(f"{'='*60}")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', ë˜ëŠ” 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("ì„ë² ë”©ì„ ë³´ë ¤ë©´ 'embed <í…ìŠ¤íŠ¸>'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("-"*60)
    
    model = None
    try:
        # ëª¨ë¸ ë¡œë“œ (ëŸ°íƒ€ì„ ë‹¤ìš´ë¡œë“œ)
        print("\nëª¨ë¸ ì¤€ë¹„ ì¤‘...")
        model_path = get_model_path(model_config["repo_id"], model_config["filename"])
        
        print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model = Llama(
            model_path=model_path,
            n_ctx=4096,
            n_threads=n_threads,
            n_gpu_layers=0,
            chat_format=model_config["chat_format"],
            embedding=embedding,
            verbose=False
        )
        print("âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n")
        
        conversation_history = []
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input("\nYou: ").strip()
                
                if not user_input:
                    continue
                
                # ì¢…ë£Œ ëª…ë ¹
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                # ì„ë² ë”© ë³´ê¸° ëª…ë ¹
                if user_input.lower().startswith('embed '):
                    text = user_input[6:].strip()
                    if text and embedding:
                        try:
                            emb = model.embed(text)
                            if hasattr(emb, 'tolist'):
                                emb_list = emb.tolist()
                            elif isinstance(emb, list):
                                emb_list = emb
                            else:
                                emb_list = list(emb)
                            
                            dim = len(emb_list)
                            print(f"\nì„ë² ë”© ì •ë³´:")
                            print(f"  í…ìŠ¤íŠ¸: {text}")
                            print(f"  ì°¨ì›: {dim}")
                            print(f"  ìƒ˜í”Œ (ì• 3ê°œ): {emb_list[:3]}")
                        except Exception as e:
                            print(f"âš  ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    elif not embedding:
                        print("âš  ì„ë² ë”©ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    continue
                
                # ëŒ€í™” ìƒì„±
                prompt = format_prompt(model_key, user_input, system_prompt)
                
                print("Assistant: ", end="", flush=True)
                start_time = time.time()
                
                output = model(
                    prompt,
                    max_tokens=100,
                    temperature=0.7,
                    top_p=0.9,
                    echo=False,
                    stop=["<|end|>", "<|im_end|>", "\n\n"]
                )
                
                elapsed = time.time() - start_time
                
                if hasattr(output, 'choices'):
                    response = output.choices[0].text.strip()
                else:
                    response = output['choices'][0]['text'].strip()
                
                print(response)
                print(f"  (ì‘ë‹µ ì‹œê°„: {elapsed:.3f}ì´ˆ)")
                
                conversation_history.append({"user": user_input, "assistant": response})
                
            except KeyboardInterrupt:
                print("\n\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except EOFError:
                print("\n\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâš  ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
        
    except Exception as e:
        print(f"âœ— ì±„íŒ… ëª¨ë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if model is not None:
            print("\nëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
            del model
            gc.collect()
            print("âœ“ ì •ë¦¬ ì™„ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬ ë° ì±„íŒ… ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--model", choices=list(MODELS.keys()) + ["all"], default="all",
                       help="ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: all, ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸)")
    parser.add_argument("--mode", choices=["benchmark", "chat"], default="benchmark",
                       help="ì‹¤í–‰ ëª¨ë“œ (ê¸°ë³¸ê°’: benchmark)")
    parser.add_argument("--embedding", action="store_true",
                       help="ì„ë² ë”© í™œì„±í™”")
    parser.add_argument("--threads", type=int, default=None,
                       help="ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ LLAMA_N_THREADS ë˜ëŠ” 4)")
    
    args = parser.parse_args()
    
    # ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì •
    if args.threads:
        n_threads = args.threads
    else:
        try:
            n_threads = int(os.getenv('LLAMA_N_THREADS', '4'))
        except:
            n_threads = 4
    
    if args.mode == "chat":
        # ì±„íŒ… ëª¨ë“œ - ëª¨ë“  ëª¨ë¸ ë˜ëŠ” ì„ íƒí•œ ëª¨ë¸
        if args.model == "all":
            models_to_chat = list(MODELS.keys())
            print(f"\nëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ì±„íŒ… ëª¨ë“œ ì‹¤í–‰: {', '.join(models_to_chat)}")
        elif args.model == "tinllama":
            # tinllama ì„ íƒ ì‹œ ë‘ TinyLlama ëª¨ë¸ ëª¨ë‘
            models_to_chat = ["tinllama", "tinllama4q"]
            print(f"\nTinyLlama ëª¨ë¸ ì±„íŒ… ëª¨ë“œ ì‹¤í–‰: {', '.join(models_to_chat)}")
        else:
            models_to_chat = [args.model]
        
        # ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ í†µì¼ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        unified_system_prompt = "Please respond in about 10 words or less."
        
        # ì—¬ëŸ¬ ëª¨ë¸ì¸ ê²½ìš° ë©€í‹° ëª¨ë¸ ì±„íŒ…, ë‹¨ì¼ ëª¨ë¸ì¸ ê²½ìš° ë‹¨ì¼ ì±„íŒ…
        if len(models_to_chat) > 1:
            multi_model_chat(models_to_chat, embedding=args.embedding, n_threads=n_threads, system_prompt=unified_system_prompt)
        else:
            interactive_chat(models_to_chat[0], embedding=args.embedding, n_threads=n_threads, system_prompt=unified_system_prompt)
    else:
        # ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ
        print("="*60)
        print("ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬")
        print("="*60)
        
        # ëª¨ë“  ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ë˜ëŠ” ì„ íƒí•œ ëª¨ë¸ë§Œ
        if args.model == "all":
            models_to_test = list(MODELS.keys())
            print(f"\nëª¨ë“  ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰: {', '.join(models_to_test)}")
        elif args.model == "tinllama":
            # tinllama ì„ íƒ ì‹œ ë‘ TinyLlama ëª¨ë¸ ëª¨ë‘ í…ŒìŠ¤íŠ¸
            models_to_test = ["tinllama", "tinllama4q"]
            print(f"\nTinyLlama ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰: {', '.join(models_to_test)}")
        else:
            models_to_test = [args.model]
        
        # ì„¤ì • ì¶œë ¥
        print(f"\nì„¤ì •:")
        print(f"  ìŠ¤ë ˆë“œ ìˆ˜: {n_threads} (í™˜ê²½ë³€ìˆ˜ LLAMA_N_THREADSë¡œ ë³€ê²½ ê°€ëŠ¥)")
        print(f"  ì§ˆë¬¸: {QUESTION}")
        
        # ëª¨ë“  ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œ (ì„ë² ë”© ë¹„í™œì„±í™” ë²„ì „)
        print(f"\n{'='*60}")
        print("1ë‹¨ê³„: ëª¨ë“  ëª¨ë¸ ë¡œë“œ (ì„ë² ë”© ë¹„í™œì„±í™”)")
        print(f"{'='*60}")
        models_no_emb = {}
        load_times_no_emb = {}
        
        for model_key in models_to_test:
            model_config = MODELS[model_key]
            print(f"\n[{model_config['name']}] ë¡œë“œ ì¤‘...")
            model_path = get_model_path(model_config["repo_id"], model_config["filename"])
            
            load_start = time.time()
            models_no_emb[model_key] = Llama(
                model_path=model_path,
                n_ctx=4096,
                n_threads=n_threads,
                n_gpu_layers=0,
                chat_format=model_config["chat_format"],
                embedding=False,
                verbose=False
            )
            load_time = time.time() - load_start
            load_times_no_emb[model_key] = load_time
            print(f"âœ“ [{model_config['name']}] ë¡œë“œ ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
        
        # ëª¨ë“  ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œ (ì„ë² ë”© í™œì„±í™” ë²„ì „)
        print(f"\n{'='*60}")
        print("2ë‹¨ê³„: ëª¨ë“  ëª¨ë¸ ë¡œë“œ (ì„ë² ë”© í™œì„±í™”)")
        print(f"{'='*60}")
        models_with_emb = {}
        load_times_with_emb = {}
        
        for model_key in models_to_test:
            model_config = MODELS[model_key]
            print(f"\n[{model_config['name']}] ë¡œë“œ ì¤‘...")
            model_path = get_model_path(model_config["repo_id"], model_config["filename"])
            
            load_start = time.time()
            models_with_emb[model_key] = Llama(
                model_path=model_path,
                n_ctx=4096,
                n_threads=n_threads,
                n_gpu_layers=0,
                chat_format=model_config["chat_format"],
                embedding=True,
                verbose=False
            )
            load_time = time.time() - load_start
            load_times_with_emb[model_key] = load_time
            print(f"âœ“ [{model_config['name']}] ë¡œë“œ ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
        
        # ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print(f"\n{'='*60}")
        print("3ë‹¨ê³„: ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print(f"{'='*60}")
        all_results = []
        
        for model_key in models_to_test:
            model_config = MODELS[model_key]
            print(f"\n{'='*60}")
            print(f"ëª¨ë¸: {model_config['name']}")
            print(f"{'='*60}")
            
            model_results = []
            
            # ì„ë² ë”© ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸
            print(f"\n[{model_config['name']}] í…ŒìŠ¤íŠ¸ 1: ì„ë² ë”© ë¹„í™œì„±í™”")
            result1 = run_benchmark_with_model(model_key, models_no_emb[model_key], embedding=False, n_threads=n_threads)
            if result1:
                result1['load_time'] = load_times_no_emb[model_key]  # ì‹¤ì œ ë¡œë“œ ì‹œê°„ ì„¤ì •
                model_results.append(result1)
            
            # ì„ë² ë”© í™œì„±í™” í…ŒìŠ¤íŠ¸
            print(f"\n[{model_config['name']}] í…ŒìŠ¤íŠ¸ 2: ì„ë² ë”© í™œì„±í™”")
            result2 = run_benchmark_with_model(model_key, models_with_emb[model_key], embedding=True, n_threads=n_threads)
            if result2:
                result2['load_time'] = load_times_with_emb[model_key]  # ì‹¤ì œ ë¡œë“œ ì‹œê°„ ì„¤ì •
                model_results.append(result2)
            
            # ëª¨ë¸ë³„ ìš”ì•½ í‘œì‹œ
            if len(model_results) == 2:
                all_results.append({
                    "model": model_key,
                    "results": model_results
                })
                
                # ëª¨ë¸ë³„ ìš”ì•½ ì¶œë ¥
                no_emb = model_results[0]
                with_emb = model_results[1]
                
                print(f"\n\n{'='*80}")
                print(f"  ğŸ“Š [{model_config['name']}] ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
                print(f"{'='*80}")
                print(f"  ìŠ¤ë ˆë“œ ìˆ˜: {no_emb['n_threads']}")
                print(f"{'='*80}")
                
                print(f"\n  {'í•­ëª©':<30} {'ì„ë² ë”© ë¹„í™œì„±í™”':<25} {'ì„ë² ë”© í™œì„±í™”':<25}")
                print("  " + "-" * 80)
                print(f"  {'ëª¨ë¸ ë¡œë“œ ì‹œê°„ (ì´ˆ)':<30} {no_emb['load_time']:<25.3f} {with_emb['load_time']:<25.3f}")
                print(f"  {'ì‘ë‹µ ìƒì„± ì‹œê°„ (ì´ˆ)':<30} {no_emb['response_time']:<25.3f} {with_emb['response_time']:<25.3f}")
                print(f"  {'ì´ ì‹œê°„ (ì´ˆ)':<30} {no_emb['load_time'] + no_emb['response_time']:<25.3f} {with_emb['load_time'] + with_emb['response_time']:<25.3f}")
                
                print(f"\n  ğŸ“ ì‘ë‹µ ê²°ê³¼:")
                print(f"     â€¢ ì„ë² ë”© ë¹„í™œì„±í™”: {no_emb['response']}")
                print(f"     â€¢ ì„ë² ë”© í™œì„±í™”:   {with_emb['response']}")
                
                # ì„±ëŠ¥ ì°¨ì´ ê³„ì‚°
                if no_emb['response_time'] > 0:
                    overhead = ((with_emb['response_time'] - no_emb['response_time']) / no_emb['response_time']) * 100
                    load_overhead = ((with_emb['load_time'] - no_emb['load_time']) / no_emb['load_time']) * 100 if no_emb['load_time'] > 0 else 0
                    print(f"\n  âš¡ ì„±ëŠ¥ ë¶„ì„:")
                    print(f"     â€¢ ì‘ë‹µ ì‹œê°„ ì˜¤ë²„í—¤ë“œ: {overhead:+.2f}%")
                    print(f"     â€¢ ë¡œë“œ ì‹œê°„ ì˜¤ë²„í—¤ë“œ: {load_overhead:+.2f}%")
                    if overhead > 0:
                        print(f"     â€¢ ì„ë² ë”© í™œì„±í™” ì‹œ ì‘ë‹µì´ {overhead:.1f}% ëŠë ¤ì§")
                    else:
                        print(f"     â€¢ ì„ë² ë”© í™œì„±í™” ì‹œ ì‘ë‹µì´ {abs(overhead):.1f}% ë¹¨ë¼ì§")
                
                # ì„ë² ë”© ì •ë³´ (ê°„ì†Œí™” - ì• 3ê°œë§Œ)
                if with_emb.get('embedding_info'):
                    emb_info = with_emb['embedding_info']
                    sample = emb_info.get('sample', [])
                    # ì •í™•íˆ ì• 3ê°œ ê°’ë§Œ í‘œì‹œ
                    if sample:
                        if isinstance(sample, list):
                            # ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í‰íƒ„í™”
                            if len(sample) > 0 and isinstance(sample[0], list):
                                flat = [item for sublist in sample for item in (sublist if isinstance(sublist, list) else [sublist])]
                                sample_display = flat[:3]
                            else:
                                sample_display = sample[:3]
                        else:
                            sample_display = [sample]
                        sample_str = f"[{', '.join(f'{x:.4f}' for x in sample_display[:3])}]"
                    else:
                        sample_str = "N/A"
                    print(f"\n  ğŸ”¢ ì„ë² ë”© ì •ë³´:")
                    print(f"     â€¢ í† í°: {emb_info['token']}")
                    print(f"     â€¢ ì°¨ì›: {emb_info['dim']}")
                    print(f"     â€¢ ìƒ˜í”Œ (ì• 3ê°œ): {sample_str}")
                
                print(f"\n{'='*80}\n")
            
            if len(model_results) == 2:
                all_results.append({
                    "model": model_key,
                    "results": model_results
                })
        
        # ì „ì²´ ë¹„êµ (ì—¬ëŸ¬ ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°ë§Œ)
        if len(all_results) > 1:
            
            print(f"\n\n{'='*80}")
            print("  ğŸ“ˆ ëª¨ë¸ ê°„ ì„±ëŠ¥ ë¹„êµ (ì„ë² ë”© ë¹„í™œì„±í™” ê¸°ì¤€)")
            print(f"{'='*80}")
            print(f"\n  {'ëª¨ë¸':<35} {'ë¡œë“œ ì‹œê°„ (ì´ˆ)':<18} {'ì‘ë‹µ ì‹œê°„ (ì´ˆ)':<18} {'ì´ ì‹œê°„ (ì´ˆ)':<18}")
            print("  " + "-" * 89)
            for model_data in all_results:
                model_key = model_data["model"]
                model_config = MODELS[model_key]
                no_emb = model_data["results"][0]
                total_time = no_emb['load_time'] + no_emb['response_time']
                print(f"  {model_config['name']:<35} {no_emb['load_time']:<18.3f} {no_emb['response_time']:<18.3f} {total_time:<18.3f}")
            
            # ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸ ì°¾ê¸° (ì„ë² ë”© ë¹„í™œì„±í™”)
            fastest_model = min(all_results, key=lambda x: x["results"][0]['load_time'] + x["results"][0]['response_time'])
            fastest_name = MODELS[fastest_model["model"]]["name"]
            fastest_time = fastest_model["results"][0]['load_time'] + fastest_model["results"][0]['response_time']
            print(f"\n  ğŸ† ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸ (ì„ë² ë”© ë¹„í™œì„±í™”): {fastest_name} (ì´ {fastest_time:.3f}ì´ˆ)")
            
            # ì„ë² ë”© í™œì„±í™” ê¸°ì¤€ ë¹„êµí‘œ
            print(f"\n\n{'='*80}")
            print("  ğŸ“ˆ ëª¨ë¸ ê°„ ì„±ëŠ¥ ë¹„êµ (ì„ë² ë”© í™œì„±í™” ê¸°ì¤€)")
            print(f"{'='*80}")
            print(f"\n  {'ëª¨ë¸':<35} {'ë¡œë“œ ì‹œê°„ (ì´ˆ)':<18} {'ì‘ë‹µ ì‹œê°„ (ì´ˆ)':<18} {'ì´ ì‹œê°„ (ì´ˆ)':<18}")
            print("  " + "-" * 89)
            for model_data in all_results:
                model_key = model_data["model"]
                model_config = MODELS[model_key]
                with_emb = model_data["results"][1]  # ì„ë² ë”© í™œì„±í™” ê²°ê³¼
                total_time = with_emb['load_time'] + with_emb['response_time']
                print(f"  {model_config['name']:<35} {with_emb['load_time']:<18.3f} {with_emb['response_time']:<18.3f} {total_time:<18.3f}")
            
            # ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸ ì°¾ê¸° (ì„ë² ë”© í™œì„±í™”)
            fastest_model_emb = min(all_results, key=lambda x: x["results"][1]['load_time'] + x["results"][1]['response_time'])
            fastest_name_emb = MODELS[fastest_model_emb["model"]]["name"]
            fastest_time_emb = fastest_model_emb["results"][1]['load_time'] + fastest_model_emb["results"][1]['response_time']
            print(f"\n  ğŸ† ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸ (ì„ë² ë”© í™œì„±í™”): {fastest_name_emb} (ì´ {fastest_time_emb:.3f}ì´ˆ)")
            print(f"\n{'='*80}\n")
        
        print(f"{'='*80}")
        print("  âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        print(f"{'='*80}\n")

if __name__ == "__main__":
    main()
